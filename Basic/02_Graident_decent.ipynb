{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7474da28",
   "metadata": {},
   "source": [
    "# Graident decent\n",
    "## 普通的梯度下降\n",
    "\n",
    "梯度下降的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb81260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784f7e1",
   "metadata": {},
   "source": [
    "### 建立模型\n",
    "\n",
    "依然使用线性模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb269c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  3.1\n"
     ]
    }
   ],
   "source": [
    "#随机在指定区间选择一个数字用来计算其梯度，此时的W是全局变量\n",
    "#w = random.uniform(0.0,4.1)\n",
    "w = 3.1\n",
    "print('w = ',w)\n",
    "\n",
    "def forward(x):\n",
    "    return x*w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12efef05",
   "metadata": {},
   "source": [
    "### Loss函数，并计算出MSE\n",
    "\n",
    ">注：MSE是**全部训练数据**Loss的平均值，下面会有随机梯度下降的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e80f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义loss函数，计算出对应w值其的error\n",
    "\n",
    "def mse(xs, ys):\n",
    "    cost = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        y_perd = forward(x)\n",
    "        cost += (y_perd - y)**2\n",
    "    return cost/len(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcad1b9",
   "metadata": {},
   "source": [
    "### optimization\n",
    "运用梯度下降法进行优化\n",
    "\n",
    "进一步解释就是，不同的w是会有不同的MSE，可以根据随机选的w来计算其所在的MSE上的梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d86b524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算w在对应点的梯度（公式涉及相关数学知识，略；并且是平均的梯度）\n",
    "def gradient(xs, ys):\n",
    "    grad = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        grad += 2*x*(x*w-y)\n",
    "    return grad/len(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f29866",
   "metadata": {},
   "source": [
    "### main 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5483053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict before training 4 12.4\n",
      "Epoch: 0  w=: 2.9973333333333336  loss= 5.646666666666668\n",
      "Epoch: 1  w=: 2.904248888888889  loss= 4.641810962962966\n",
      "Epoch: 2  w=: 2.8198523259259263  loss= 3.8157749142650226\n",
      "Epoch: 3  w=: 2.743332775506173  loss= 3.1367365695220406\n",
      "Epoch: 4  w=: 2.6739550497922635  loss= 2.5785368706613165\n",
      "Epoch: 5  w=: 2.611052578478319  loss= 2.1196719093222973\n",
      "Epoch: 6  w=: 2.554021004487009  loss= 1.742464517103344\n",
      "Epoch: 7  w=: 2.5023123774015548  loss= 1.432383275926375\n",
      "Epoch: 8  w=: 2.455429888844076  loss= 1.1774827142904087\n",
      "Epoch: 9  w=: 2.412923099218629  loss= 0.9679431237117955\n",
      "Epoch: 10  w=: 2.374383609958224  loss= 0.7956922673854828\n",
      "Epoch: 11  w=: 2.3394411396954564  loss= 0.6540944078916399\n",
      "Epoch: 12  w=: 2.3077599666572137  loss= 0.537694674149502\n",
      "Epoch: 13  w=: 2.2790357031025406  loss= 0.44200891969196343\n",
      "Epoch: 14  w=: 2.25299237081297  loss= 0.3633509768276697\n",
      "Epoch: 15  w=: 2.2293797495370926  loss= 0.2986906518846472\n",
      "Epoch: 16  w=: 2.2079709729136305  loss= 0.2455369909892636\n",
      "Epoch: 17  w=: 2.188560348775025  loss= 0.20184231934832972\n",
      "Epoch: 18  w=: 2.170961382889356  loss= 0.16592335727407578\n",
      "Epoch: 19  w=: 2.155004987153016  loss= 0.13639637405072447\n",
      "Epoch: 20  w=: 2.1405378550187346  loss= 0.1121238815307646\n",
      "Epoch: 21  w=: 2.1274209885503192  loss= 0.09217081390191194\n",
      "Epoch: 22  w=: 2.1155283629522894  loss= 0.0757685055079893\n",
      "Epoch: 23  w=: 2.104745715743409  loss= 0.06228507901670085\n",
      "Epoch: 24  w=: 2.0949694489406907  loss= 0.05120110317746204\n",
      "Epoch: 25  w=: 2.086105633706226  loss= 0.04208958241645947\n",
      "Epoch: 26  w=: 2.078069107893645  loss= 0.03459950739443692\n",
      "Epoch: 27  w=: 2.0707826578235715  loss= 0.02844233283411121\n",
      "Epoch: 28  w=: 2.064176276426705  loss= 0.023380861693321178\n",
      "Epoch: 29  w=: 2.058186490626879  loss= 0.019220107461318508\n",
      "Epoch: 30  w=: 2.052755751501704  loss= 0.015799782560202254\n",
      "Epoch: 31  w=: 2.047831881361545  loss= 0.01298812347704447\n",
      "Epoch: 32  w=: 2.0433675724344673  loss= 0.010676814748062957\n",
      "Epoch: 33  w=: 2.0393199323405837  loss= 0.008776816248007552\n",
      "Epoch: 34  w=: 2.0356500719887958  loss= 0.007214933036584349\n",
      "Epoch: 35  w=: 2.032322731936508  loss= 0.005930995619762832\n",
      "Epoch: 36  w=: 2.029305943622434  loss= 0.0048755419992503\n",
      "Epoch: 37  w=: 2.026570722217673  loss= 0.0040079122141392495\n",
      "Epoch: 38  w=: 2.0240907881440235  loss= 0.0032946819694541544\n",
      "Epoch: 39  w=: 2.021842314583915  loss= 0.00270837500920103\n",
      "Epoch: 40  w=: 2.0198036985560828  loss= 0.0022264046297859274\n",
      "Epoch: 41  w=: 2.017955353357515  loss= 0.0018302035570009095\n",
      "Epoch: 42  w=: 2.01627952037748  loss= 0.0015045086662350618\n",
      "Epoch: 43  w=: 2.014760098475582  loss= 0.0012367729906970124\n",
      "Epoch: 44  w=: 2.0133824892845276  loss= 0.0010166823660413898\n",
      "Epoch: 45  w=: 2.012133456951305  loss= 0.0008357580907689772\n",
      "Epoch: 46  w=: 2.011001000969183  loss= 0.0006870302954161449\n",
      "Epoch: 47  w=: 2.009974240878726  loss= 0.0005647694375118603\n",
      "Epoch: 48  w=: 2.009043311730045  loss= 0.00046426557849861923\n",
      "Epoch: 49  w=: 2.0081992693019073  loss= 0.0003816469395516009\n",
      "Epoch: 50  w=: 2.0074340041670626  loss= 0.00031373074639759626\n",
      "Epoch: 51  w=: 2.0067401637781366  loss= 0.00025790061712754837\n",
      "Epoch: 52  w=: 2.0061110818255106  loss= 0.00021200576952848686\n",
      "Epoch: 53  w=: 2.005540714188463  loss= 0.00017427816503106598\n",
      "Epoch: 54  w=: 2.0050235808642065  loss= 0.00014326439735175983\n",
      "Epoch: 55  w=: 2.0045547133168804  loss= 0.00011776970192970639\n",
      "Epoch: 56  w=: 2.004129606740638  loss= 9.681192919518639e-05\n",
      "Epoch: 57  w=: 2.0037441767781785  loss= 7.95837085508399e-05\n",
      "Epoch: 58  w=: 2.003394720278882  loss= 6.542134548250229e-05\n",
      "Epoch: 59  w=: 2.0030778797195197  loss= 5.37792536019789e-05\n",
      "Epoch: 60  w=: 2.0027906109456977  loss= 4.4208936649875635e-05\n",
      "Epoch: 61  w=: 2.0025301539240994  loss= 3.634171076782202e-05\n",
      "Epoch: 62  w=: 2.002294006224517  loss= 2.9874501438299136e-05\n",
      "Epoch: 63  w=: 2.0020798989768953  loss= 2.4558167937904415e-05\n",
      "Epoch: 64  w=: 2.001885775072385  loss= 2.0187905519089275e-05\n",
      "Epoch: 65  w=: 2.0017097693989623  loss= 1.6595355576936726e-05\n",
      "Epoch: 66  w=: 2.001550190921726  loss= 1.3642119855596565e-05\n",
      "Epoch: 67  w=: 2.0014055064356984  loss= 1.1214428837738825e-05\n",
      "Epoch: 68  w=: 2.001274325835033  loss= 9.218758923683216e-06\n",
      "Epoch: 69  w=: 2.0011553887570965  loss= 7.578229557887319e-06\n",
      "Epoch: 70  w=: 2.001047552473101  loss= 6.229641506783651e-06\n",
      "Epoch: 71  w=: 2.000949780908945  loss= 5.121042191531902e-06\n",
      "Epoch: 72  w=: 2.0008611346907768  loss= 4.209724283315873e-06\n",
      "Epoch: 73  w=: 2.0007807621196374  loss= 3.460580459743591e-06\n",
      "Epoch: 74  w=: 2.0007078909884712  loss= 2.8447509414832108e-06\n",
      "Epoch: 75  w=: 2.0006418211628807  loss= 2.338511707274966e-06\n",
      "Epoch: 76  w=: 2.0005819178543454  loss= 1.9223605572338e-06\n",
      "Epoch: 77  w=: 2.000527605521273  loss= 1.5802658162937165e-06\n",
      "Epoch: 78  w=: 2.0004783623392877  loss= 1.299048735029848e-06\n",
      "Epoch: 79  w=: 2.000433715187621  loss= 1.0678757956942398e-06\n",
      "Epoch: 80  w=: 2.000393235103443  loss= 8.778413652075017e-07\n",
      "Epoch: 81  w=: 2.000356533160455  loss= 7.216246173714483e-07\n",
      "Epoch: 82  w=: 2.0003232567321456  loss= 5.932075076855253e-07\n",
      "Epoch: 83  w=: 2.000293086103812  loss= 4.876429360948249e-07\n",
      "Epoch: 84  w=: 2.00026573140079  loss= 4.008641664897492e-07\n",
      "Epoch: 85  w=: 2.000240929803383  loss= 3.295281610404424e-07\n",
      "Epoch: 86  w=: 2.000218443021734  loss= 2.708867940716161e-07\n",
      "Epoch: 87  w=: 2.000198055006372  loss= 2.2268098413975115e-07\n",
      "Epoch: 88  w=: 2.000179569872444  loss= 1.8305366589547576e-07\n",
      "Epoch: 89  w=: 2.0001628100176827  loss= 1.5047824908483396e-07\n",
      "Epoch: 90  w=: 2.0001476144160324  loss= 1.236998086699279e-07\n",
      "Epoch: 91  w=: 2.000133837070536  loss= 1.016867404959299e-07\n",
      "Epoch: 92  w=: 2.0001213456106193  loss= 8.3591020098444e-08\n",
      "Epoch: 93  w=: 2.0001100200202946  loss= 6.87155336772989e-08\n",
      "Epoch: 94  w=: 2.000099751485067  loss= 5.648722270633144e-08\n",
      "Epoch: 95  w=: 2.000090441346461  loss= 4.643500760788268e-08\n",
      "Epoch: 96  w=: 2.0000820001541246  loss= 3.817164003171265e-08\n",
      "Epoch: 97  w=: 2.0000743468064064  loss= 3.1378784623493174e-08\n",
      "Epoch: 98  w=: 2.000067407771142  loss= 2.579475557320241e-08\n",
      "Epoch: 99  w=: 2.0000611163791686  loss= 2.120443551474596e-08\n",
      "predict after training 4 8.000244465516674\n"
     ]
    }
   ],
   "source": [
    "# 建立两个数组储存相关数据\n",
    "loss_list = []\n",
    "epoch_list = []\n",
    "\n",
    "# 进行100次更新，即100次训练 \n",
    "print('predict before training', 4, forward(4))\n",
    "for epoch in range(100):\n",
    "    epoch_list.append(epoch)\n",
    "    \n",
    "    mse_val = mse(x_data, y_data)\n",
    "    loss_list.append(mse_val)\n",
    "    \n",
    "    grad_val = gradient(x_data, y_data)\n",
    "    \n",
    "    # 注意！！ 这里的0.01(learning rate)是 hyperparameter\n",
    "    # 有时候不能取太大，训练多次loss可能会上升\n",
    "    # 根据定义更新w的数值\n",
    "    w -= 0.01* grad_val\n",
    "\n",
    "    print('Epoch:', epoch, ' w=:', w, ' loss=', mse_val)\n",
    "\n",
    "print('predict after training', 4, forward(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9317945e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_list, loss_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259c7e7b",
   "metadata": {},
   "source": [
    "## 随机梯度下降\n",
    "每次用随机的一个样本的的loss进行更新\n",
    "但不能进行并行计算\n",
    "相当于进行batch的更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5863144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x,y):\n",
    "    y_perd = forward(x)\n",
    "    \n",
    "    # 只用一组的loss就进行参数的更新\n",
    "    return (y_perd - y)**2\n",
    "\n",
    "def gradient_ar(x, y):\n",
    "    return 2*x*(x*w - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7cfa5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict before training 4 12.4\n",
      "\t grad 1.0 2.0 2.2\n",
      "\t grad 2.0 4.0 8.624000000000002\n",
      "\t grad 3.0 6.0 17.85168000000001\n",
      "progess 0  w= 2.8132432 loss 5.952280521116158\n",
      "\t grad 1.0 2.0 1.6264864\n",
      "\t grad 2.0 4.0 6.375826688\n",
      "\t grad 3.0 6.0 13.197961244160005\n",
      "progess 1  w= 2.6012404566784 loss 3.2534107807216595\n",
      "\t grad 1.0 2.0 1.2024809133568004\n",
      "\t grad 2.0 4.0 4.713725180358658\n",
      "\t grad 3.0 6.0 9.757411123342418\n",
      "progess 2  w= 2.4445042845078215 loss 1.778256530512292\n",
      "\t grad 1.0 2.0 0.8890085690156431\n",
      "\t grad 2.0 4.0 3.4849135905413213\n",
      "\t grad 3.0 6.0 7.213771132420538\n",
      "progess 3  w= 2.3286273515880467 loss 0.9719634259059629\n",
      "\t grad 1.0 2.0 0.6572547031760934\n",
      "\t grad 2.0 4.0 2.5764384364502853\n",
      "\t grad 3.0 6.0 5.333227563452088\n",
      "progess 4  w= 2.242958144557262 loss 0.5312579400603665\n",
      "\t grad 1.0 2.0 0.4859162891145239\n",
      "\t grad 2.0 4.0 1.904791853328934\n",
      "\t grad 3.0 6.0 3.9429191363908913\n",
      "progess 5  w= 2.1796218717689184 loss 0.29037615135992795\n",
      "\t grad 1.0 2.0 0.35924374353783683\n",
      "\t grad 2.0 4.0 1.40823547466832\n",
      "\t grad 3.0 6.0 2.915047432563421\n",
      "progess 6  w= 2.1327966052612224 loss 0.15871444532014425\n",
      "\t grad 1.0 2.0 0.2655932105224448\n",
      "\t grad 2.0 4.0 1.0411253852479838\n",
      "\t grad 3.0 6.0 2.1551295474633285\n",
      "progess 7  w= 2.098178123828885 loss 0.08675049598703853\n",
      "\t grad 1.0 2.0 0.1963562476577696\n",
      "\t grad 2.0 4.0 0.7697164908184568\n",
      "\t grad 3.0 6.0 1.5933131359942028\n",
      "progess 8  w= 2.0725842650841804 loss 0.04741627984029508\n",
      "\t grad 1.0 2.0 0.14516853016836073\n",
      "\t grad 2.0 4.0 0.5690606382599732\n",
      "\t grad 3.0 6.0 1.1779555211981432\n",
      "progess 9  w= 2.0536624181879155 loss 0.025916896131972525\n",
      "\t grad 1.0 2.0 0.10732483637583101\n",
      "\t grad 2.0 4.0 0.4207133585932574\n",
      "\t grad 3.0 6.0 0.8708766522880396\n",
      "progess 10  w= 2.0396732697153444 loss 0.01416571496915824\n",
      "\t grad 1.0 2.0 0.0793465394306887\n",
      "\t grad 2.0 4.0 0.3110384345682995\n",
      "\t grad 3.0 6.0 0.6438495595563865\n",
      "progess 11  w= 2.029330924379791 loss 0.00774272812475713\n",
      "\t grad 1.0 2.0 0.05866184875958158\n",
      "\t grad 2.0 4.0 0.2299544471375583\n",
      "\t grad 3.0 6.0 0.4760057055747495\n",
      "progess 12  w= 2.021684704365072 loss 0.004232037630605244\n",
      "\t grad 1.0 2.0 0.043369408730144166\n",
      "\t grad 2.0 4.0 0.17000808222216435\n",
      "\t grad 3.0 6.0 0.3519167301998767\n",
      "progess 13  w= 2.01603176215355 loss 0.0023131565797319706\n",
      "\t grad 1.0 2.0 0.03206352430709991\n",
      "\t grad 2.0 4.0 0.12568901528383236\n",
      "\t grad 3.0 6.0 0.26017626163753427\n",
      "progess 14  w= 2.0118524741412656 loss 0.0012643302894242865\n",
      "\t grad 1.0 2.0 0.023704948282531113\n",
      "\t grad 2.0 4.0 0.09292339726752274\n",
      "\t grad 3.0 6.0 0.19235143234377006\n",
      "progess 15  w= 2.0087626763623274 loss 0.0006910604732780479\n",
      "\t grad 1.0 2.0 0.01752535272465483\n",
      "\t grad 2.0 4.0 0.06869938268064857\n",
      "\t grad 3.0 6.0 0.14220772214894062\n",
      "progess 16  w= 2.006478351786785 loss 0.00037772137686006586\n",
      "\t grad 1.0 2.0 0.012956703573570039\n",
      "\t grad 2.0 4.0 0.05079027800839597\n",
      "\t grad 3.0 6.0 0.1051358754773819\n",
      "progess 17  w= 2.004789523216192 loss 0.00020645579374596736\n",
      "\t grad 1.0 2.0 0.00957904643238372\n",
      "\t grad 2.0 4.0 0.037549862014945035\n",
      "\t grad 3.0 6.0 0.07772821437093569\n",
      "progess 18  w= 2.003540951988009 loss 0.00011284506883245245\n",
      "\t grad 1.0 2.0 0.007081903976017934\n",
      "\t grad 2.0 4.0 0.027761063585991508\n",
      "\t grad 3.0 6.0 0.05746540162300562\n",
      "progess 19  w= 2.0026178682961593 loss 6.167910974432336e-05\n",
      "\t grad 1.0 2.0 0.005235736592318574\n",
      "\t grad 2.0 4.0 0.020524087441888383\n",
      "\t grad 3.0 6.0 0.04248486100470927\n",
      "progess 20  w= 2.0019354214457703 loss 3.3712705554728464e-05\n",
      "\t grad 1.0 2.0 0.0038708428915406046\n",
      "\t grad 2.0 4.0 0.015173704134838317\n",
      "\t grad 3.0 6.0 0.03140956755911617\n",
      "progess 21  w= 2.0014308802999152 loss 1.8426765894165596e-05\n",
      "\t grad 1.0 2.0 0.002861760599830454\n",
      "\t grad 2.0 4.0 0.011218101551335735\n",
      "\t grad 3.0 6.0 0.023221470211264972\n",
      "progess 22  w= 2.001057866976291 loss 1.0071742855744832e-05\n",
      "\t grad 1.0 2.0 0.002115733952582133\n",
      "\t grad 2.0 4.0 0.008293677094123098\n",
      "\t grad 3.0 6.0 0.017167911584838436\n",
      "progess 23  w= 2.000782093749976 loss 5.505035703761486e-06\n",
      "\t grad 1.0 2.0 0.0015641874999516858\n",
      "\t grad 2.0 4.0 0.006131614999809187\n",
      "\t grad 3.0 6.0 0.01269244304960182\n",
      "progess 24  w= 2.000578211294482 loss 3.008954709599227e-06\n",
      "\t grad 1.0 2.0 0.001156422588963757\n",
      "\t grad 2.0 4.0 0.004533176548736861\n",
      "\t grad 3.0 6.0 0.009383675455886475\n",
      "progess 25  w= 2.0004274785485463 loss 1.644641185204128e-06\n",
      "\t grad 1.0 2.0 0.0008549570970926013\n",
      "\t grad 2.0 4.0 0.003351431820604489\n",
      "\t grad 3.0 6.0 0.006937463868652571\n",
      "progess 26  w= 2.000316040020683 loss 8.989316520588025e-07\n",
      "\t grad 1.0 2.0 0.0006320800413659455\n",
      "\t grad 2.0 4.0 0.002477753762153867\n",
      "\t grad 3.0 6.0 0.005128950287659251\n",
      "progess 27  w= 2.000233652179771 loss 4.913400700041198e-07\n",
      "\t grad 1.0 2.0 0.0004673043595415649\n",
      "\t grad 2.0 4.0 0.0018318330894047108\n",
      "\t grad 3.0 6.0 0.00379189449506967\n",
      "progess 28  w= 2.000172741860331 loss 2.6855775279600796e-07\n",
      "\t grad 1.0 2.0 0.00034548372066200983\n",
      "\t grad 2.0 4.0 0.001354296184995718\n",
      "\t grad 3.0 6.0 0.0028033931029476378\n",
      "progess 29  w= 2.0001277101302453 loss 1.4678889630535633e-07\n",
      "\t grad 1.0 2.0 0.00025542026049052424\n",
      "\t grad 2.0 4.0 0.0010012474211222866\n",
      "\t grad 3.0 6.0 0.0020725821617233464\n",
      "progess 30  w= 2.000094417631812 loss 8.023220277272865e-08\n",
      "\t grad 1.0 2.0 0.00018883526362412084\n",
      "\t grad 2.0 4.0 0.000740234233408188\n",
      "\t grad 3.0 6.0 0.0015322848631527108\n",
      "progess 31  w= 2.00006980408821 loss 4.385349657760653e-08\n",
      "\t grad 1.0 2.0 0.00013960817642022505\n",
      "\t grad 2.0 4.0 0.0005472640515655769\n",
      "\t grad 3.0 6.0 0.0011328365867342427\n",
      "progess 32  w= 2.0000516070000627 loss 2.3969542099226153e-08\n",
      "\t grad 1.0 2.0 0.0001032140001253623\n",
      "\t grad 2.0 4.0 0.00040459888048971493\n",
      "\t grad 3.0 6.0 0.0008375196826140296\n",
      "progess 33  w= 2.00003815367443 loss 1.3101325852731691e-08\n",
      "\t grad 1.0 2.0 7.630734886010515e-05\n",
      "\t grad 2.0 4.0 0.00029912480753324644\n",
      "\t grad 3.0 6.0 0.0006191883515889174\n",
      "progess 34  w= 2.0000282074693505 loss 7.160951944437223e-09\n",
      "\t grad 1.0 2.0 5.641493870101044e-05\n",
      "\t grad 2.0 4.0 0.00022114655970639774\n",
      "\t grad 3.0 6.0 0.0004577733785957605\n",
      "progess 35  w= 2.0000208541205806 loss 3.914049106717712e-09\n",
      "\t grad 1.0 2.0 4.1708241161231285e-05\n",
      "\t grad 2.0 4.0 0.000163496305351174\n",
      "\t grad 3.0 6.0 0.00033843735207916836\n",
      "progess 36  w= 2.0000154177015945 loss 2.139349702119611e-09\n",
      "\t grad 1.0 2.0 3.083540318904454e-05\n",
      "\t grad 2.0 4.0 0.00012087478049949141\n",
      "\t grad 3.0 6.0 0.0002502107956345867\n",
      "progess 37  w= 2.000011398491801 loss 1.1693305380568018e-09\n",
      "\t grad 1.0 2.0 2.279698360219129e-05\n",
      "\t grad 2.0 4.0 8.936417572158462e-05\n",
      "\t grad 3.0 6.0 0.0001849838437468776\n",
      "progess 38  w= 2.0000084270417706 loss 6.39135297056428e-10\n",
      "\t grad 1.0 2.0 1.6854083541240072e-05\n",
      "\t grad 2.0 4.0 6.606800748087949e-05\n",
      "\t grad 3.0 6.0 0.00013676077548829824\n",
      "progess 39  w= 2.0000062302131054 loss 3.4933999805031757e-10\n",
      "\t grad 1.0 2.0 1.2460426210836317e-05\n",
      "\t grad 2.0 4.0 4.8844870747188907e-05\n",
      "\t grad 3.0 6.0 0.00010110888245051797\n",
      "progess 40  w= 2.0000046060713115 loss 1.9094303634979662e-10\n",
      "\t grad 1.0 2.0 9.21214262294967e-06\n",
      "\t grad 2.0 4.0 3.611159908345485e-05\n",
      "\t grad 3.0 6.0 7.47510101017923e-05\n",
      "progess 41  w= 2.0000034053237936 loss 1.0436607125423868e-10\n",
      "\t grad 1.0 2.0 6.810647587229823e-06\n",
      "\t grad 2.0 4.0 2.669773854080404e-05\n",
      "\t grad 3.0 6.0 5.5264318774561616e-05\n",
      "progess 42  w= 2.0000025175967444 loss 5.704464030723435e-11\n",
      "\t grad 1.0 2.0 5.035193488822642e-06\n",
      "\t grad 2.0 4.0 1.9737958474763673e-05\n",
      "\t grad 3.0 6.0 4.085757404403978e-05\n",
      "progess 43  w= 2.0000018612894843 loss 3.1179586899871334e-11\n",
      "\t grad 1.0 2.0 3.7225789686345934e-06\n",
      "\t grad 2.0 4.0 1.4592509558042366e-05\n",
      "\t grad 3.0 6.0 3.0206494788131977e-05\n",
      "progess 44  w= 2.0000013760736515 loss 1.7042208248459237e-11\n",
      "\t grad 1.0 2.0 2.752147302942376e-06\n",
      "\t grad 2.0 4.0 1.078841742696568e-05\n",
      "\t grad 3.0 6.0 2.2332024069982026e-05\n",
      "progess 45  w= 2.0000010173477634 loss 9.314968244626837e-12\n",
      "\t grad 1.0 2.0 2.0346955267314115e-06\n",
      "\t grad 2.0 4.0 7.97600646507135e-06\n",
      "\t grad 3.0 6.0 1.651033337779495e-05\n",
      "progess 46  w= 2.0000007521374097 loss 5.0913961454585976e-12\n",
      "\t grad 1.0 2.0 1.5042748193749844e-06\n",
      "\t grad 2.0 4.0 5.896757290457799e-06\n",
      "\t grad 3.0 6.0 1.2206287586025155e-05\n",
      "progess 47  w= 2.0000005560642125 loss 2.7828666753445566e-12\n",
      "\t grad 1.0 2.0 1.112128424907155e-06\n",
      "\t grad 2.0 4.0 4.359543424925505e-06\n",
      "\t grad 3.0 6.0 9.024254890022121e-06\n",
      "progess 48  w= 2.0000004111049448 loss 1.5210654804036675e-12\n",
      "\t grad 1.0 2.0 8.222098895060981e-07\n",
      "\t grad 2.0 4.0 3.223062766011253e-06\n",
      "\t grad 3.0 6.0 6.671739928520992e-06\n",
      "progess 49  w= 2.000000303934819 loss 8.313873691089326e-13\n",
      "\t grad 1.0 2.0 6.078696381806026e-07\n",
      "\t grad 2.0 4.0 2.3828489830179933e-06\n",
      "\t grad 3.0 6.0 4.932497397192037e-06\n",
      "progess 50  w= 2.000000224702659 loss 4.544215652463727e-13\n",
      "\t grad 1.0 2.0 4.4940531829240626e-07\n",
      "\t grad 2.0 4.0 1.7616688481325582e-06\n",
      "\t grad 3.0 6.0 3.6466545143554185e-06\n",
      "progess 51  w= 2.000000166125372 loss 2.4837875337373467e-13\n",
      "\t grad 1.0 2.0 3.3225074425047296e-07\n",
      "\t grad 2.0 4.0 1.302422916182877e-06\n",
      "\t grad 3.0 6.0 2.696015432235299e-06\n",
      "progess 52  w= 2.000000122818481 loss 1.3575941416398036e-13\n",
      "\t grad 1.0 2.0 2.456369623260457e-07\n",
      "\t grad 2.0 4.0 9.628968911101765e-07\n",
      "\t grad 3.0 6.0 1.9931965677955077e-06\n",
      "progess 53  w= 2.000000090801177 loss 7.420368361586121e-14\n",
      "\t grad 1.0 2.0 1.8160235359943044e-07\n",
      "\t grad 2.0 4.0 7.118812277440156e-07\n",
      "\t grad 3.0 6.0 1.4735941427090893e-06\n",
      "progess 54  w= 2.0000000671304 loss 4.0558415323347195e-14\n",
      "\t grad 1.0 2.0 1.342607998111589e-07\n",
      "\t grad 2.0 4.0 5.263023368229369e-07\n",
      "\t grad 3.0 6.0 1.0894458348786884e-06\n",
      "progess 55  w= 2.0000000496303105 loss 2.2168509507915107e-14\n",
      "\t grad 1.0 2.0 9.926062105591882e-08\n",
      "\t grad 2.0 4.0 3.8910163624450433e-07\n",
      "\t grad 3.0 6.0 8.054403863866355e-07\n",
      "progess 56  w= 2.0000000366922843 loss 1.2116913461140545e-14\n",
      "\t grad 1.0 2.0 7.338456864403042e-08\n",
      "\t grad 2.0 4.0 2.876675075924595e-07\n",
      "\t grad 3.0 6.0 5.954717465783688e-07\n",
      "progess 57  w= 2.000000027127046 loss 6.622889666563261e-15\n",
      "\t grad 1.0 2.0 5.4254091885752587e-08\n",
      "\t grad 2.0 4.0 2.1267603855790185e-07\n",
      "\t grad 3.0 6.0 4.4023939693715874e-07\n",
      "progess 58  w= 2.0000000200553503 loss 3.619953669850526e-15\n",
      "\t grad 1.0 2.0 4.0110700538775745e-08\n",
      "\t grad 2.0 4.0 1.5723394497513254e-07\n",
      "\t grad 3.0 6.0 3.2547426087603526e-07\n",
      "progess 59  w= 2.000000014827161 loss 1.9786022851629887e-15\n",
      "\t grad 1.0 2.0 2.9654321664907002e-08\n",
      "\t grad 2.0 4.0 1.162449407843269e-07\n",
      "\t grad 3.0 6.0 2.406270276367195e-07\n",
      "progess 60  w= 2.000000010961898 loss 1.0814688808991823e-15\n",
      "\t grad 1.0 2.0 2.192379611187789e-08\n",
      "\t grad 2.0 4.0 8.594128075856133e-08\n",
      "\t grad 3.0 6.0 1.7789844797277965e-07\n",
      "progess 61  w= 2.0000000081042626 loss 5.911116713768344e-16\n",
      "\t grad 1.0 2.0 1.6208525188687872e-08\n",
      "\t grad 2.0 4.0 6.353741710540817e-08\n",
      "\t grad 3.0 6.0 1.3152245692538145e-07\n",
      "progess 62  w= 2.000000005991579 loss 3.2309114742719887e-16\n",
      "\t grad 1.0 2.0 1.1983157577333259e-08\n",
      "\t grad 2.0 4.0 4.6973976708386544e-08\n",
      "\t grad 3.0 6.0 9.723612848233643e-08\n",
      "progess 63  w= 2.000000004429646 loss 1.7659587184030965e-16\n",
      "\t grad 1.0 2.0 8.859291966700766e-09\n",
      "\t grad 2.0 4.0 3.472842280416444e-08\n",
      "\t grad 3.0 6.0 7.188782902289859e-08\n",
      "progess 64  w= 2.00000000327489 loss 9.652415740203677e-17\n",
      "\t grad 1.0 2.0 6.549780273701344e-09\n",
      "\t grad 2.0 4.0 2.567513845974645e-08\n",
      "\t grad 3.0 6.0 5.314753437346553e-08\n",
      "progess 65  w= 2.0000000024211655 loss 5.275838098357622e-17\n",
      "\t grad 1.0 2.0 4.842330980636689e-09\n",
      "\t grad 2.0 4.0 1.898193602301035e-08\n",
      "\t grad 3.0 6.0 3.9292611830887836e-08\n",
      "progess 66  w= 2.0000000017899966 loss 2.88367903036212e-17\n",
      "\t grad 1.0 2.0 3.5799931907831706e-09\n",
      "\t grad 2.0 4.0 1.4033574302629859e-08\n",
      "\t grad 3.0 6.0 2.9049498806443808e-08\n",
      "progess 67  w= 2.000000001323366 loss 1.5761671300456706e-17\n",
      "\t grad 1.0 2.0 2.6467317226774867e-09\n",
      "\t grad 2.0 4.0 1.0375188708167116e-08\n",
      "\t grad 3.0 6.0 2.1476644462836703e-08\n",
      "progess 68  w= 2.0000000009783805 loss 8.615058015994147e-18\n",
      "\t grad 1.0 2.0 1.956760975474481e-09\n",
      "\t grad 2.0 4.0 7.670504231782616e-09\n",
      "\t grad 3.0 6.0 1.5877944292697066e-08\n",
      "progess 69  w= 2.0000000007233285 loss 4.7088352496510516e-18\n",
      "\t grad 1.0 2.0 1.4466570164017867e-09\n",
      "\t grad 2.0 4.0 5.670894864806542e-09\n",
      "\t grad 3.0 6.0 1.1738753968870697e-08\n",
      "progess 70  w= 2.0000000005347656 loss 2.5737693120616913e-18\n",
      "\t grad 1.0 2.0 1.0695311303265953e-09\n",
      "\t grad 2.0 4.0 4.19256096506615e-09\n",
      "\t grad 3.0 6.0 8.678604501710652e-09\n",
      "progess 71  w= 2.0000000003953584 loss 1.4067744702546054e-18\n",
      "\t grad 1.0 2.0 7.907168253495911e-10\n",
      "\t grad 2.0 4.0 3.0996112343473214e-09\n",
      "\t grad 3.0 6.0 6.4161955748431865e-09\n",
      "progess 72  w= 2.0000000002922933 loss 7.689183625720764e-19\n",
      "\t grad 1.0 2.0 5.845866013487466e-10\n",
      "\t grad 2.0 4.0 2.2915784825272567e-09\n",
      "\t grad 3.0 6.0 4.743565540366035e-09\n",
      "progess 73  w= 2.0000000002160956 loss 4.202757200920196e-19\n",
      "\t grad 1.0 2.0 4.3219117173975974e-10\n",
      "\t grad 2.0 4.0 1.6941896774369525e-09\n",
      "\t grad 3.0 6.0 3.506970713829105e-09\n",
      "progess 74  w= 2.000000000159762 loss 2.2971501637089296e-19\n",
      "\t grad 1.0 2.0 3.1952396284395945e-10\n",
      "\t grad 2.0 4.0 1.2525340764568682e-09\n",
      "\t grad 3.0 6.0 2.592742021079175e-09\n",
      "progess 75  w= 2.000000000118114 loss 1.2555848913521112e-19\n",
      "\t grad 1.0 2.0 2.362279261092226e-10\n",
      "\t grad 2.0 4.0 9.260148203793506e-10\n",
      "\t grad 3.0 6.0 1.9168506781852557e-09\n",
      "progess 76  w= 2.000000000087323 loss 6.862781620214365e-20\n",
      "\t grad 1.0 2.0 1.7464607537931442e-10\n",
      "\t grad 2.0 4.0 6.846114786185353e-10\n",
      "\t grad 3.0 6.0 1.4171490647640894e-09\n",
      "progess 77  w= 2.000000000064559 loss 3.751080913971005e-20\n",
      "\t grad 1.0 2.0 1.29118049585486e-10\n",
      "\t grad 2.0 4.0 5.06144459677671e-10\n",
      "\t grad 3.0 6.0 1.0477219092308587e-09\n",
      "progess 78  w= 2.0000000000477294 loss 2.0502967183008173e-20\n",
      "\t grad 1.0 2.0 9.545875201411036e-11\n",
      "\t grad 2.0 4.0 3.7419667364702036e-10\n",
      "\t grad 3.0 6.0 7.745910579615156e-10\n",
      "progess 79  w= 2.000000000035287 loss 1.1206383952677791e-20\n",
      "\t grad 1.0 2.0 7.057376905095225e-11\n",
      "\t grad 2.0 4.0 2.76649814168195e-10\n",
      "\t grad 3.0 6.0 5.726619178858527e-10\n",
      "progess 80  w= 2.000000000026088 loss 6.12533289741039e-21\n",
      "\t grad 1.0 2.0 5.217604126528386e-11\n",
      "\t grad 2.0 4.0 2.0452972648854484e-10\n",
      "\t grad 3.0 6.0 4.233733363889769e-10\n",
      "progess 81  w= 2.000000000019287 loss 3.3478239384227232e-21\n",
      "\t grad 1.0 2.0 3.857358876757644e-11\n",
      "\t grad 2.0 4.0 1.5120704688342812e-10\n",
      "\t grad 3.0 6.0 3.1299762781600293e-10\n",
      "progess 82  w= 2.000000000014259 loss 1.829824593292805e-21\n",
      "\t grad 1.0 2.0 2.851763269973162e-11\n",
      "\t grad 2.0 4.0 1.1178968861713656e-10\n",
      "\t grad 3.0 6.0 2.3140955818234943e-10\n",
      "progess 83  w= 2.000000000010542 loss 1.000163963770248e-21\n",
      "\t grad 1.0 2.0 2.1083579326841573e-11\n",
      "\t grad 2.0 4.0 8.264677830993605e-11\n",
      "\t grad 3.0 6.0 1.7107915084579872e-10\n",
      "progess 84  w= 2.0000000000077938 loss 5.4668504466075305e-22\n",
      "\t grad 1.0 2.0 1.5587531265737198e-11\n",
      "\t grad 2.0 4.0 6.110312256168982e-11\n",
      "\t grad 3.0 6.0 1.2648015967897663e-10\n",
      "progess 85  w= 2.000000000005762 loss 2.9879640646389934e-22\n",
      "\t grad 1.0 2.0 1.1524114995609125e-11\n",
      "\t grad 2.0 4.0 4.517630713962717e-11\n",
      "\t grad 3.0 6.0 9.351452945338679e-11\n",
      "progess 86  w= 2.00000000000426 loss 1.6335108425201588e-22\n",
      "\t grad 1.0 2.0 8.520295580183301e-12\n",
      "\t grad 2.0 4.0 3.339906129440351e-11\n",
      "\t grad 3.0 6.0 6.913936090313655e-11\n",
      "progess 87  w= 2.0000000000031495 loss 8.927305677102607e-23\n",
      "\t grad 1.0 2.0 6.298961352513288e-12\n",
      "\t grad 2.0 4.0 2.469136006766348e-11\n",
      "\t grad 3.0 6.0 5.1111115340063407e-11\n",
      "progess 88  w= 2.0000000000023284 loss 4.87851273756868e-23\n",
      "\t grad 1.0 2.0 4.6567194544877566e-12\n",
      "\t grad 2.0 4.0 1.8253842881676974e-11\n",
      "\t grad 3.0 6.0 3.778310997404333e-11\n",
      "progess 89  w= 2.0000000000017213 loss 2.666554647938015e-23\n",
      "\t grad 1.0 2.0 3.4425795547576854e-12\n",
      "\t grad 2.0 4.0 1.3493206552084303e-11\n",
      "\t grad 3.0 6.0 2.7929658585890138e-11\n",
      "progess 90  w= 2.0000000000012723 loss 1.4572472885410328e-23\n",
      "\t grad 1.0 2.0 2.5446311724408588e-12\n",
      "\t grad 2.0 4.0 9.976020010071807e-12\n",
      "\t grad 3.0 6.0 2.064481918750971e-11\n",
      "progess 91  w= 2.0000000000009406 loss 7.96223265163349e-24\n",
      "\t grad 1.0 2.0 1.8811618929248652e-12\n",
      "\t grad 2.0 4.0 7.37543359718984e-12\n",
      "\t grad 3.0 6.0 1.5267787034645153e-11\n",
      "progess 92  w= 2.0000000000006954 loss 4.352777491689404e-24\n",
      "\t grad 1.0 2.0 1.3908874052503961e-12\n",
      "\t grad 2.0 4.0 5.453415496958769e-12\n",
      "\t grad 3.0 6.0 1.1286971357549191e-11\n",
      "progess 93  w= 2.0000000000005143 loss 2.380126668544775e-24\n",
      "\t grad 1.0 2.0 1.028510610012745e-12\n",
      "\t grad 2.0 4.0 4.0323300254385686e-12\n",
      "\t grad 3.0 6.0 8.345324431502377e-12\n",
      "progess 94  w= 2.00000000000038 loss 1.3005602645580524e-24\n",
      "\t grad 1.0 2.0 7.602807272633072e-13\n",
      "\t grad 2.0 4.0 2.9807267765136203e-12\n",
      "\t grad 3.0 6.0 6.17106366007647e-12\n",
      "progess 95  w= 2.000000000000281 loss 7.119469669619632e-25\n",
      "\t grad 1.0 2.0 5.622169396701793e-13\n",
      "\t grad 2.0 4.0 2.2026824808563106e-12\n",
      "\t grad 3.0 6.0 4.5563552930616424e-12\n",
      "progess 96  w= 2.0000000000002074 loss 3.865418435582958e-25\n",
      "\t grad 1.0 2.0 4.147793219999585e-13\n",
      "\t grad 2.0 4.0 1.6271428648906294e-12\n",
      "\t grad 3.0 6.0 3.367972567502875e-12\n",
      "progess 97  w= 2.000000000000153 loss 2.1167031353252277e-25\n",
      "\t grad 1.0 2.0 3.064215547965432e-13\n",
      "\t grad 2.0 4.0 1.2008172234345693e-12\n",
      "\t grad 3.0 6.0 2.48334686148155e-12\n",
      "progess 98  w= 2.0000000000001132 loss 1.1511373873347093e-25\n",
      "\t grad 1.0 2.0 2.2648549702353193e-13\n",
      "\t grad 2.0 4.0 8.881784197001252e-13\n",
      "\t grad 3.0 6.0 1.8385293287792592e-12\n",
      "progess 99  w= 2.000000000000084 loss 6.362636517150593e-26\n",
      "predict after training 4 8.000000000000336\n"
     ]
    }
   ],
   "source": [
    "# 建立两个数组储存相关数据\n",
    "loss_list = []\n",
    "epoch_list = []\n",
    "num = 0\n",
    "\n",
    "# 进行100次更新，即100次训练 \n",
    "print('predict before training', 4, forward(4))\n",
    "for epoch in range(100):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        epoch_list.append(num)\n",
    "        num += 1\n",
    "        grad = gradient_ar(x, y)\n",
    "        \n",
    "        # 注！！每一次更新需要用到上次的W值\n",
    "        w = w - 0.01*grad\n",
    "        print('\\t grad', x, y, grad)\n",
    "        \n",
    "        l = loss(x, y)\n",
    "        loss_list.append(l)\n",
    "        \n",
    "    print('progess', epoch, ' w=', w, 'loss', l)\n",
    "print('predict after training', 4, forward(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f68c013e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_list, loss_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989831ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
